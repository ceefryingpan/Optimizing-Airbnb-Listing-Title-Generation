{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "airbnbtitlepred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnsx25hxyKkg",
        "colab_type": "text"
      },
      "source": [
        "**Imports**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDT535WB1GEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "69523ced-0294-4c0f-cd30-3abb25108e68"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.metrics import categorical_accuracy\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "#Downloading existing universal models\n",
        "nltk.download('popular')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf-UXV2v6xr4",
        "colab_type": "text"
      },
      "source": [
        "**Code for Cleaning up Titles and Averaging the Vectors**\n",
        "1. Remove all punctuation, numbers in titles\n",
        "2. Tokenize titles by space\n",
        "3. Import word2vec/glove vectors and average them\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYWnRas8aLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateAverageVector(test_string):\n",
        "  # def stripTitle(doc):\n",
        "  #   strip_punc = re.sub(r'[^\\w\\s]','',doc)\n",
        "  #   return re.sub(r'\\w*\\d\\w*', '', strip_punc).strip()\n",
        "  def stripTitle(doc):\n",
        "    return re.sub('[^A-Za-z0-9]+', ' ', doc)\n",
        "  test_string = stripTitle(test_string)\n",
        "  summed_vectors = np.zeros(96)\n",
        "  word_count = 0;\n",
        "  for token in nlp(test_string):\n",
        "    if token.has_vector:\n",
        "      summed_vectors = np.add(summed_vectors, token.vector)\n",
        "      word_count = word_count + 1\n",
        "  return np.true_divide(summed_vectors, word_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gRsgpH9e-Qd",
        "colab_type": "text"
      },
      "source": [
        "# Code for POS Filtering\n",
        "1. Separate title into POS using NLTK\n",
        "2. Remove words in list of POS\n",
        "3. Return filtered title\n",
        "\n",
        "POS Tags\n",
        "POS tag list:\n",
        "\n",
        "CC coordinating conjunction\n",
        "CD cardinal digit\n",
        "DT determiner\n",
        "EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "FW foreign word\n",
        "IN preposition/subordinating conjunction\n",
        "JJ adjective 'big'\n",
        "JJR adjective, comparative 'bigger'\n",
        "JJS adjective, superlative 'biggest'\n",
        "LS list marker 1)\n",
        "MD modal could, will\n",
        "NN noun, singular 'desk'\n",
        "NNS noun plural 'desks'\n",
        "NNP proper noun, singular 'Harrison'\n",
        "NNPS proper noun, plural 'Americans'\n",
        "PDT predeterminer 'all the kids'\n",
        "POS possessive ending parent's\n",
        "PRP personal pronoun I, he, she\n",
        "PRP$ possessive pronoun my, his, hers\n",
        "RB adverb very, silently,\n",
        "RBR adverb, comparative better\n",
        "RBS adverb, superlative best\n",
        "RP particle give up\n",
        "TO to go 'to' the store.\n",
        "UH interjection errrrrrrrm\n",
        "VB verb, base form take\n",
        "VBD verb, past tense took\n",
        "VBG verb, gerund/present participle taking\n",
        "VBN verb, past participle taken\n",
        "VBP verb, sing. present, non-3d take\n",
        "VBZ verb, 3rd person sing. present takes\n",
        "WDT wh-determiner which\n",
        "WP wh-pronoun who, what\n",
        "WP$ possessive wh-pronoun whose\n",
        "WRB wh-abverb where, when\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30omlru-A1gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nouns = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\n",
        "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "adjv = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
        "\n",
        "def filterTitle(title, POS):\n",
        "    # POS is list of parts of speech wanted included in the filtered in title\n",
        "\n",
        "    # sent_tokenize is one of instances of  \n",
        "    # PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
        "      \n",
        "    # tokenized = sent_tokenize(title)\n",
        "    # print(tokenized)\n",
        "    \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string \n",
        "    wordsList = nltk.word_tokenize(title)\n",
        "\n",
        "    # removing stop words from wordList \n",
        "    wordsList = [w for w in wordsList if not w in stop_words] \n",
        "\n",
        "    #  Using a Tagger. Which is part-of-speech  \n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "    filtered = [word[0] for word in tagged if word[1] in POS]\n",
        "    filteredtitle = ' '.join(filtered[:10])\n",
        "    return filteredtitle\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMnk3ATmfRwl",
        "colab_type": "text"
      },
      "source": [
        "**Creating LSTM Input**\n",
        "\n",
        "The method cleans up each title, and stacks them to create a n x 96 matrix of word vectors for each title, with n being the number of prespecified words in a title.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvB9jc6XMXqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "def generate_ltsm_input(titles):  # titles is list of Airbnb titles (strings)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    def stripTitle(doc):\n",
        "      strip_punc = re.sub(r'[^\\w\\s]','',doc)\n",
        "      return re.sub(r'\\w*\\d\\w*', '', strip_punc).strip()\n",
        "    \n",
        "    lengths = [len(title.split()) for title in titles]\n",
        "    plen = 10\n",
        "    titlevecs = np.zeros(shape=(1, plen, 96))  # baseline shape\n",
        "\n",
        "    # print(titlevecs)\n",
        "    for title in titles:\n",
        "        title = stripTitle(title)\n",
        "        summed_vectors = np.zeros(96)\n",
        "        title = filterTitle(title, adjv)  # input nouns, verbs, or adjv \n",
        "        tokens = nlp(title)\n",
        "        for count in range(plen):  # loop through tokens until end, then fill with null tokens\n",
        "            if count < len(tokens) and tokens[count].has_vector:\n",
        "                summed_vectors = np.vstack((summed_vectors, tokens[count].vector))\n",
        "            else:\n",
        "                summed_vectors = np.vstack((summed_vectors, np.zeros(96)))\n",
        "        titlevecs = np.vstack((titlevecs, summed_vectors[1:][:].reshape(1, plen, 96)))  # add title to the stack of titles\n",
        "\n",
        "        # count = 0\n",
        "        # for token in nlp(title):\n",
        "        #     if token.has_vector:\n",
        "        #         summed_vectors = np.vstack((summed_vectors, token.vector))\n",
        "        #     else:\n",
        "        #         summed_vectors = np.vstack((summed_vectors, np.zeros(96)))\n",
        "        #     count += 1\n",
        "        # if count <= plen:\n",
        "        #     for iterations in range(plen - count):\n",
        "        #         summed_vectors = np.vstack((summed_vectors, np.zeros(96)))  ## null token\n",
        "\n",
        "    # print(titlevecs)\n",
        "    return titlevecs[1:][:][:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HZrd0lzbvJk",
        "colab_type": "text"
      },
      "source": [
        "**Code for Creating Training Data**\n",
        "1. Loop through dataset(s) for titles. \n",
        "2. Generate title vectors for each title and create X_train. \n",
        "3. Normalize average reviews and create Y_train. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1hG42cWiO29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "05e81b4f-d37f-4dcb-f505-449d49df1f05"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "brooklynhigh = pd.read_csv('Brooklyn_high.csv', encoding = \"ISO-8859-1\")\n",
        "brooklynlow = pd.read_csv('Brooklyn_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "manhattanhigh = pd.read_csv('Manhattan_high.csv', encoding = \"ISO-8859-1\")\n",
        "manhattanlow = pd.read_csv('Manhattan_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "queenshigh = pd.read_csv('Queens_high.csv', encoding = \"ISO-8859-1\")\n",
        "queenslow = pd.read_csv('Queens_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "bronxhigh = pd.read_csv('Bronx_high.csv', encoding = \"ISO-8859-1\")\n",
        "bronxlow = pd.read_csv('Bronx_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "statenhigh = pd.read_csv('StatenIsland_high.csv', encoding = \"ISO-8859-1\")\n",
        "statenlow = pd.read_csv('StatenIsland_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "dataset = brooklynhigh  # put the dataset you want to use here\n",
        "titles = [title for title in dataset['name']]\n",
        "print(len(titles))\n",
        "combined_data = generate_ltsm_input(titles)  # dataset\n",
        "print(combined_data.shape)\n",
        "\n",
        "# combined_data = pd.DataFrame(summed_vectors)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3fa549aa0e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbrooklynhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Brooklyn_high.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbrooklynlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Brooklyn_low.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Brooklyn_high.csv' does not exist: b'Brooklyn_high.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFsqRGVaiTp7",
        "colab_type": "text"
      },
      "source": [
        "**Creating Categorical Output**\n",
        "\n",
        "The following method determines what are the best discrete bins to put each continuous output value into."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNdtPCRmiTKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def convert_categorical(dataset):  # dataset must be sorted\n",
        "    base = dataset.min()\n",
        "    diff = dataset.max() - base\n",
        "    summed_vectors = np.zeros(5)\n",
        "    for iteration in range(len(dataset)):\n",
        "        entry = np.zeros(5)\n",
        "        entry[int(iteration / (len(dataset) / 5))] = 1\n",
        "        summed_vectors = np.vstack((summed_vectors, entry))\n",
        "\n",
        "    # for point in dataset:\n",
        "    #     entry = np.zeros(max_val)\n",
        "    #     entry[int(point) - 1 if int(point) - 1 > 0 else 0] += 1\n",
        "    #     summed_vectors = np.vstack((summed_vectors, entry))\n",
        "    return summed_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPrd_kFAdtbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_df = combined_data\n",
        "Y_df = dataset['reviews_per_month']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.1, random_state=99)\n",
        "print(Y_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA0LWG2SCmqK",
        "colab_type": "text"
      },
      "source": [
        "**Code for Training Keras Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxz7-WPzCunE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # put the dataset you want to use her\n",
        "def eval_lstm(dataset):\n",
        "  titles = [title for title in dataset['name']]\n",
        "  print(len(titles))\n",
        "  combined_data = generate_ltsm_input(titles) \n",
        "  X_df = combined_data\n",
        "  Y_df = dataset['reviews_per_month']\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.25, random_state=99)  \n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, input_shape=(X_df.shape[1], 96)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(Y_df.shape[1], activation='sigmoid'))\n",
        "  model.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  # Fit model in batches\n",
        "  model.fit(X_train, Y_train, nb_epoch=100, batch_size=100)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHZqMDoSoPsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "brooklynhigh = pd.read_csv('Brooklyn_high.csv', encoding = \"ISO-8859-1\")\n",
        "brooklynhigh_model = eval_lstm(brooklynhigh)\n",
        "brooklynhigh_model.predict(X_test[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiJCnoliYW94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "brooklynhigh = pd.read_csv('Brooklyn_high.csv', encoding = \"ISO-8859-1\")\n",
        "brooklynlow = pd.read_csv('Brooklyn_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "manhattanhigh = pd.read_csv('Manhattan_high.csv', encoding = \"ISO-8859-1\")\n",
        "manhattanlow = pd.read_csv('Manhattan_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "queenshigh = pd.read_csv('Queens_high.csv', encoding = \"ISO-8859-1\")\n",
        "queenslow = pd.read_csv('Queens_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "bronxhigh = pd.read_csv('Bronx_high.csv', encoding = \"ISO-8859-1\")\n",
        "bronxlow = pd.read_csv('Bronx_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "statenhigh = pd.read_csv('StatenIsland_high.csv', encoding = \"ISO-8859-1\")\n",
        "statenlow = pd.read_csv('StatenIsland_low.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "datasets = [brooklynhigh, brooklynlow, manhattanhigh, manhattanlow, queenshigh, queenslow, \n",
        "            bronxhigh, bronxlow, statenhigh, statenlow]\n",
        "\n",
        "brooklynhigh_model = eval_lstm(brooklynhigh)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXipiWnzgX2j",
        "colab_type": "text"
      },
      "source": [
        "**Code for Testing Title Replacements Based on Best Possible Word**\n",
        "1. Loop through each word in given title.\n",
        "2. For each word, generate list of 10 best/closest words\n",
        "3. Replace current word with each closest word and score the new title based on RNN\n",
        "4. Return best title based on best score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPmjB8gKC9Ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "outputId": "c0cb21f2-950c-4909-9d44-1cb063e11e2b"
      },
      "source": [
        "# import spacy\n",
        "!pip install sense2vec\n",
        "# from sense2vec import Sense2VecComponent\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# s2v = Sense2VecComponent(nlp.vocab).from_disk(\"s2v_old\")\n",
        "# nlp.add_pipe(s2v)\n",
        "\n",
        "def optimal_replacement(title):\n",
        "    bestscore = 0\n",
        "    besttitle = \"\"\n",
        "    individual = title.split()\n",
        "    for index in range(len(individual)):\n",
        "        word = individual[index]\n",
        "        doc = nlp(word)\n",
        "        # freq = doc[:]._.s2v_freq\n",
        "        # vector = doc[:]._.s2v_vec\n",
        "        if not doc[:]._.in_s2v:\n",
        "            individual[index] = word\n",
        "            continue\n",
        "        most_similar = doc[:]._.s2v_most_similar(10)\n",
        "        for similar in most_similar:\n",
        "            replacement = similar[0][0]\n",
        "            individual[index] = replacement\n",
        "            testtitle = ' '.join(individual)\n",
        "            score = RNN/LTSM(testtitle)  # TODO: replace with LSTM run\n",
        "            if score > bestscore:\n",
        "                bestscore = score\n",
        "                besttitle = testtitle\n",
        "        individual[index] = word\n",
        "    return besttitle, bestscore\n",
        "\n",
        "# print(most_similar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sense2vec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/bf/5b776ad825e30e6fa5e86a74711caa84bde65b22047868e588290367253f/sense2vec-1.0.2.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.0,>=2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from sense2vec) (1.0.1)\n",
            "Collecting catalogue>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/8e/9391f722c58dc202cb7980a3a1f0e2499cc91e1fbda2c17632dad1b6e299/catalogue-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from sense2vec) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from sense2vec) (1.17.5)\n",
            "Requirement already satisfied: importlib_metadata>=0.20 in /usr/local/lib/python3.6/dist-packages (from sense2vec) (1.5.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec) (2.21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec) (45.1.0)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 44.6MB/s \n",
            "\u001b[?25hCollecting blis<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec) (2.0.3)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib_metadata>=0.20->sense2vec) (2.2.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy<3.0.0,>=2.2.3->sense2vec) (4.28.1)\n",
            "Building wheels for collected packages: sense2vec\n",
            "  Building wheel for sense2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sense2vec: filename=sense2vec-1.0.2-py2.py3-none-any.whl size=34997 sha256=79edfaaa8004683461dc71f554b243b71f6916c28df1cde02e7d7cf4af51ee0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/86/94/9dbbd58956c20435a4ca918ea5a0dfdec3ad4e9568c3f41b3e\n",
            "Successfully built sense2vec\n",
            "\u001b[31mERROR: spacy 2.2.3 has requirement catalogue<1.1.0,>=0.0.7, but you'll have catalogue 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: preshed, blis, thinc, catalogue, spacy, sense2vec\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.9\n",
            "    Uninstalling spacy-2.1.9:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OURsjS0dcv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}